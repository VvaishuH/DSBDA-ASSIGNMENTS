{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvagEwYKm9Z0ekwaa7TSAB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kcpyopiZVyYT","executionInfo":{"status":"ok","timestamp":1712557448571,"user_tz":-330,"elapsed":436,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}},"outputId":"ed41e839-9414-4c72-a7ba-52327a511874"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["text= \"Tokenization is the first step in text analytics. Theprocess of breaking down a text paragraph into smaller chunksbsuch as words or sentences is called Tokenization.\""],"metadata":{"id":"C-v9EbT3YNeU","executionInfo":{"status":"ok","timestamp":1712557484660,"user_tz":-330,"elapsed":461,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize\n","tokenized_text= sent_tokenize(text)\n","print(tokenized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hHuwei_KYQg7","executionInfo":{"status":"ok","timestamp":1712557503346,"user_tz":-330,"elapsed":3,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}},"outputId":"bb0e1fde-f2d8-465d-9bad-a676997bf0ec"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization is the first step in text analytics.', 'Theprocess of breaking down a text paragraph into smaller chunksbsuch as words or sentences is called Tokenization.']\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","tokenized_word=word_tokenize(text)\n","print(tokenized_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9AxBpMF_Ya8K","executionInfo":{"status":"ok","timestamp":1712557629760,"user_tz":-330,"elapsed":440,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}},"outputId":"cd6f7d11-3e59-4e28-f97e-2ce1ba87c1f4"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'Theprocess', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunksbsuch', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"]}]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","stop_words=set(stopwords.words(\"english\"))\n","print(stop_words)\n","text= \"How to remove stop words with NLTK library in Python?\"\n","\n","tokens = word_tokenize(text.lower())\n","filtered_text=[]\n","for w in tokens:\n","     if w not in stop_words:\n","          filtered_text.append(w)\n","print(\"Tokenized Sentence:\",tokens)\n","print(\"Filterd Sentence:\",filtered_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TqA4UtHaY5tK","executionInfo":{"status":"ok","timestamp":1712557904296,"user_tz":-330,"elapsed":5,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}},"outputId":"e8c6bd58-30bf-4fb5-becc-2d026c9d42ac"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["{'of', 'should', 'were', 'when', 'i', 'by', 'needn', 'while', 'ourselves', \"that'll\", \"you've\", 'there', 'those', 'at', 'on', \"you're\", \"wouldn't\", 'out', 'any', 'couldn', 'being', \"weren't\", 'against', 'a', 'doesn', 're', \"mustn't\", 'm', 'what', 'be', 'now', \"you'd\", 'my', \"shan't\", 'does', 'you', \"wasn't\", 'had', 'other', 'haven', 'this', 'hasn', 'd', 'too', 've', 'all', 'down', \"don't\", 'ain', 'where', 'mightn', \"hadn't\", 'have', 'and', 'own', 'don', 'only', 'wasn', 'whom', 'but', 'didn', 'his', 't', 'are', 'its', 'for', \"doesn't\", 'o', 'her', 'won', 'more', 'myself', 'between', \"didn't\", 'until', 'it', 'same', 'your', 'each', \"mightn't\", 'why', 'they', 'is', 'an', 'he', 'yourself', 'aren', \"couldn't\", \"won't\", 'theirs', 'the', 'through', \"you'll\", 'doing', 'shan', 'to', 'ma', 'y', 'we', 'can', 'if', 'that', 'some', 'under', 'as', 'in', 'how', 'was', 'weren', 'itself', 'will', \"it's\", 'because', 'once', 'below', 'about', 'over', \"needn't\", \"she's\", 'after', 'above', 'off', 'himself', 'both', 's', 'few', 'most', 'into', 'who', 'again', 'did', 'hadn', 'yourselves', 'before', 'our', 'then', 'll', 'having', 'she', 'such', 'from', 'hers', 'nor', 'than', 'during', 'wouldn', 'ours', \"should've\", 'isn', 'do', 'been', 'me', 'herself', 'up', 'their', \"haven't\", 'him', \"hasn't\", 'with', 'shouldn', 'so', 'themselves', 'them', \"shouldn't\", 'not', 'am', 'has', 'just', 'here', 'no', 'further', 'mustn', 'these', 'very', 'yours', \"aren't\", 'which', \"isn't\", 'or'}\n","Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python', '?']\n","Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python', '?']\n"]}]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n","ps =PorterStemmer()\n","for w in e_words:\n","      rootWord=ps.stem(w)\n","print(rootWord)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NjiurdYJY_ii","executionInfo":{"status":"ok","timestamp":1712557970987,"user_tz":-330,"elapsed":415,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}},"outputId":"9d8cf51f-923e-4718-fbcb-6eaeb25caf89"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["wait\n"]}]},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","text = \"studies studying cries cry\"\n","tokenization = nltk.word_tokenize(text)\n","for w in tokenization:\n","       print(\"Lemma for {} is {}\".format(w,\n","wordnet_lemmatizer.lemmatize(w)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_x5cj1a1aNBi","executionInfo":{"status":"ok","timestamp":1712558007988,"user_tz":-330,"elapsed":1728,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}},"outputId":"a3636392-48c5-4ed4-f79a-5499897e651e"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemma for studies is study\n","Lemma for studying is studying\n","Lemma for cries is cry\n","Lemma for cry is cry\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","data=\"The pink sweater fit her perfectly\"\n","words=word_tokenize(data)\n","for word in words:\n","       print(nltk.pos_tag([word]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2-YSMAmaR5S","executionInfo":{"status":"ok","timestamp":1712558035426,"user_tz":-330,"elapsed":400,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}},"outputId":"59058f19-a3e0-4647-a456-337844e3def2"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'DT')]\n","[('pink', 'NN')]\n","[('sweater', 'NN')]\n","[('fit', 'NN')]\n","[('her', 'PRP$')]\n","[('perfectly', 'RB')]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"id":"_YOArcCXacxB","executionInfo":{"status":"ok","timestamp":1712558099920,"user_tz":-330,"elapsed":1162,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["documentA = 'Jupiter is the largest Planet'\n","documentB = 'Mars is the fourth planet from the Sun'"],"metadata":{"id":"gu08iNUwasUK","executionInfo":{"status":"ok","timestamp":1712558112889,"user_tz":-330,"elapsed":3,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["bagOfWordsA = documentA.split(' ')\n","bagOfWordsB = documentB.split(' ')"],"metadata":{"id":"6uUK8ta_av1y","executionInfo":{"status":"ok","timestamp":1712558126393,"user_tz":-330,"elapsed":452,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"],"metadata":{"id":"P7uGOsO_ay8a","executionInfo":{"status":"ok","timestamp":1712558138102,"user_tz":-330,"elapsed":481,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["numOfWordsA = dict.fromkeys(uniqueWords, 0)\n","for word in bagOfWordsA:\n","   numOfWordsA[word] += 1\n","numOfWordsB = dict.fromkeys(uniqueWords, 0)\n","for word in bagOfWordsB:\n","    numOfWordsB[word] += 1"],"metadata":{"id":"deyYh3BJa1zR","executionInfo":{"status":"ok","timestamp":1712558177411,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def computeTF(wordDict, bagOfWords):\n","   tfDict = {}\n","   bagOfWordsCount = len(bagOfWords)\n","   for word, count in wordDict.items():\n","    wordtfDict[word] = count / float(bagOfWordsCount)\n","    return tfDict\n","    tfA = computeTF(numOfWordsA, bagOfWordsA)\n","    tfB = computeTF(numOfWordsB, bagOfWordsB)"],"metadata":{"id":"MRRvAJuVa8qZ","executionInfo":{"status":"ok","timestamp":1712558552956,"user_tz":-330,"elapsed":3,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["def computeTFIDF(tfBagOfWords, idfs):\n","  tfidf = {}\n","  for word, val in tfBagOfWords.items():\n","    tfidf[word] = val * idfs[word]\n","    return tfidf\n","    tfidfA = computeTFIDF(tfA, idfs)\n","    tfidfB = computeTFIDF(tfB, idfs)\n","    df = pd.DataFrame([tfidfA, tfidfB])\n"],"metadata":{"id":"PiPtoOaybE5R","executionInfo":{"status":"ok","timestamp":1712558884478,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vaishnavi Harihar","userId":"09860996889247503377"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"urJp_4oYcmiL"},"execution_count":null,"outputs":[]}]}